{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code up SIRD model with deaths\n",
    "# fit using adjoint method\n",
    "# see how well it does on simulated data\n",
    "# see how well it does on real data\n",
    "# might have to play with noise parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 4)\n"
     ]
    }
   ],
   "source": [
    "soldat = onp.loadtxt('soldat.csv',delimiter=',')\n",
    "print(soldat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as np\n",
    "import jax.nn\n",
    "from jax import grad, jit, jacobian, random, vmap, lax\n",
    "from jax.ops import index, index_update\n",
    "from jax.experimental import ode\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "nn = 1000\n",
    "npts = 1001\n",
    "n = npts\n",
    "lentheta = 3\n",
    "d = 4\n",
    "dt = 0.1\n",
    "tint = np.arange(n)*dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = (beta, gamma, mu)\n",
    "def sird(x, t, theta):\n",
    "    stheta = np.exp(theta)\n",
    "    sdot = -stheta[0]*x[0]*x[1]/nn\n",
    "    idot = stheta[0]*x[0]*x[1]/nn - stheta[1]*x[1] - stheta[2]*x[1]\n",
    "    rdot = stheta[1]*x[1]\n",
    "    ddot = stheta[2]*x[1]\n",
    "    return np.array([sdot, idot, rdot, ddot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ -0.03261938, -10.98294937,   6.59488508,   4.42068367], dtype=float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sird(np.array([3.,4.,5.,6.]),0.5,np.array([1.0,0.5,0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just-in-time (JIT) compiled version\n",
    "fsird = jit(sird)\n",
    "\n",
    "# use automatic differentiation and JIT together\n",
    "mygradsird = jacobian(sird, 0)\n",
    "fmygradsird = jit(mygradsird)\n",
    "\n",
    "# use automatic differentiation and JIT together\n",
    "mygradsirdtheta = jacobian(sird, 2)\n",
    "fmygradsirdtheta = jit(mygradsirdtheta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scipy.optimize\n",
    "# z should have (xinit, curtheta)\n",
    "# note that the t variable is **not** passed in as z[0]\n",
    "def newlagwithgrad(xinit, curtheta):\n",
    "    # solves the forward ODE using our current estimates of xinit and curtheta\n",
    "    fsirdI = lambda y, t: fsird(y, t, curtheta)\n",
    "    x = lax.stop_gradient(ode.odeint(fsirdI, t=tint, rtol=1e-9, atol=1e-9, y0=xinit)).T\n",
    "    \n",
    "    # set up adjoint ODE\n",
    "    fadj = lambda y, t: -np.matmul(y, fmygradsird(y, t, curtheta))\n",
    "    icmat = np.eye(d)\n",
    "    adjtint = np.array([0, dt])\n",
    "    \n",
    "    # function that solves the adjoint ODE once for one initial condition\n",
    "    @jit\n",
    "    def solonce(y0):\n",
    "        adjsol = lax.stop_gradient(ode.odeint(fadj, t=adjtint, rtol=1e-9, atol=1e-9, y0=y0))\n",
    "        return adjsol[1,:]\n",
    "    \n",
    "    # this is to solve the adjoint ODE for all initial conditions in the icmat **at once**\n",
    "    propagator = vmap(solonce, in_axes=(0))(icmat) # + (1e-6)*np.eye(d)\n",
    "    backprop = lax.stop_gradient(np.linalg.inv(propagator))\n",
    "\n",
    "    yminusx = lax.stop_gradient(y - x)\n",
    "    \n",
    "    @jit\n",
    "    def growlamb(i, lamb):\n",
    "        lambplus = np.matmul(lamb[i,:], backprop)\n",
    "        outlamb = index_update(lamb, index[i+1, :], lambplus + yminusx[:,(npts-2-i)])\n",
    "        return outlamb\n",
    "        \n",
    "    initlamb = np.vstack([np.expand_dims(yminusx[:,(npts-1)],0), np.zeros((npts-1, d))])\n",
    "    lambminus = lax.fori_loop(0, npts-1, growlamb, initlamb)\n",
    "            \n",
    "    # compute current value of lagrangian\n",
    "    allxdot = np.hstack([(x[:,[1]]-x[:,[0]]), (x[:,2:] - x[:,:-2])/2, (x[:,[npts-2]]-x[:,[npts-3]])])/dt\n",
    "    \n",
    "    @jit\n",
    "    def goodfun(i, lag):\n",
    "        f = fsird(x[:, i], tint[i], curtheta)\n",
    "        lag1 = lag + np.dot(lambminus[npts-1-i], allxdot[:,i]-f)*dt\n",
    "        return lag1\n",
    "        \n",
    "    lag = lax.fori_loop(0, npts-1, goodfun, 0.0)\n",
    "    lag += np.sum(np.square(x - y))/2.0\n",
    "    \n",
    "    # compute gradients using lamb (solution of adjoint ODE)\n",
    "    # gradient of L with respect to parameters theta\n",
    "    initgradtheta = np.zeros(lentheta)\n",
    "    \n",
    "    @jit\n",
    "    def gt1i(i, gt):\n",
    "        g = fmygradsirdtheta(x[:, i], tint[i], curtheta).reshape((d, lentheta))  # nabla_theta f\n",
    "        gradtheta = gt - np.matmul(lambminus[npts-1-i],g)*dt\n",
    "        return gradtheta\n",
    "    \n",
    "    gradtheta = lax.fori_loop(0, npts-1, gt1i, initgradtheta)\n",
    "    gradx0 = -lambminus[npts-2]\n",
    "    \n",
    "    return lag, gradx0, gradtheta, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagwithgrad = jit(newlagwithgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.91982837 -0.02253266 -1.10660533]\n",
      "431715.2375958681\n",
      "636098698.9880548 [0.05394294 0.9777193  0.33067961]\n",
      "635979044.044479 [0.06433861 1.24946965 0.25869111]\n",
      "635902108.1676689 [0.07783262 1.50739896 0.21437367]\n",
      "635841572.6798483 [0.09608699 1.74800841 0.18482067]\n",
      "635780492.4281824 [0.12245382 1.97289995 0.16371181]\n",
      "635700557.1266948 [0.16449799 2.18489957 0.14778625]\n",
      "635560459.4906447 [0.24372799 2.38741639 0.13520529]\n",
      "635156635.1909875 [0.45941204 2.58642636 0.12474232]\n",
      "130005930.29482687 [2.34798569 0.92865073 0.23872606]\n",
      "2088.965546889067 [0.40107127 0.03499123 0.0050688 ]\n",
      "0.00011085715008056455 [0.40000004 0.035      0.005     ]\n",
      "-1.245929431122475e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n",
      "-1.2832043571753997e-07 [0.4   0.035 0.005]\n"
     ]
    }
   ],
   "source": [
    "# adjoint solver with GD (gradient descent)\n",
    "y = soldat.T\n",
    "\n",
    "# take as initial guess x = y\n",
    "theta0 = -4*onp.abs(onp.random.normal(size=lentheta))\n",
    "print(theta0)\n",
    "x0 = y[:,0] # + 1e-8*onp.random.rand(d)\n",
    "\n",
    "maxiters = 30000\n",
    "step = 1e-9\n",
    "\n",
    "x = x0.copy()\n",
    "theta = theta0.copy()\n",
    "\n",
    "lag, gradx0, gradtheta, xest = lagwithgrad(x, theta)\n",
    "print(onp.linalg.norm(gradtheta))\n",
    "\n",
    "# mys = 1e-2\n",
    "for i in range(maxiters):\n",
    "    lag, gradx0, gradtheta, xest = lagwithgrad(x, theta)\n",
    "    if i % 1000 == 0:\n",
    "        print(lag, onp.exp(theta))\n",
    "\n",
    "    theta -= step*gradtheta\n",
    "    # x0 -= step*gradx0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
